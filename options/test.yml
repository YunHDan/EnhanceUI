name: test
model_type: CodeFormerModel
num_gpu: 1
manual_seed: 42

# dataset and data loader settings
datasets:
  val:
    name: Personal_Test
    type: PairedImageDataset
    dataroot_lq: ./uploads
    dataroot_gt: ./uploads
    io_backend:
      type: disk
    scale: 1


# network structures
network_g:
  type: CodeFormer
  dim_embd: 256
  n_head: 8
  mamba_layers: 6
  transformer_layers: 2
  codebook_size: 1024
  latent_size: 256
  fuse_encoder_block: [12, 8, 4]
  fuse_generator_block: [10, 13, 16]
  fix_modules: ['dae','macr', 'hq_encoder', 'quantize','generator']
  vqgan_path: ~
  img_size: 256
  nf: 64
  ch_mult: [1, 2, 2, 4, 4]
  res_blocks: 2
  d_state: 32
  mlp_ratio: 4
  L: 32
  hidden_list: [128, 128, 128]


network_vqgan: # this config is needed if no pre-calculated latent
  type: VQAutoEncoder
  img_size: 256
  nf: 64
  ch_mult: [1, 2, 2, 4, 4]
  quantizer: 'nearest'
  codebook_size: 1024

# path
path:
  pretrain_network_hq: ~
  strict_load_hq: false
  pretrain_network_g: net_g_latest.pth
  param_key_g: params_ema
  strict_load_g: true
  resume_state: ~

train:
  use_hq_feat_loss: false
  feat_loss_weight: 0.2
  cross_entropy_loss: false
  entropy_loss_weight: 2.0
  fidelity_weight: 1.0

  optim_g:
    type: Adam
    lr: !!float 2e-4
    weight_decay: 0
    betas: [0.9, 0.99]

  scheduler:
    type: CosineAnnealingRestartCyclicLR
    periods: [20000, 20000, 51000, 30000]
    restart_weights: [1, 0, 0, 0]
    eta_mins: [0.0003, 0.0003, 0.00025, 0.0002]

  total_iter: 121000

  warmup_iter: -1  # no warm up
  ema_decay: 0.997

  pixel_opt:
    type: L1Loss
    loss_weight: 2.0 # 5.0
    reduction: mean

  perceptual_opt:
    type: LPIPSLoss
    loss_weight: 1.0
    use_input_norm: true
    range_norm: true

  ssim_opt:
    loss_weight: 1.0
    normalize: true

  use_adaptive_weight: true

  net_g_start_iter: 0
  manual_seed: 0

# validation settings
val:
  val_freq: !!float 1000    # 2000
  save_img: true
  suffix: ~

# logging settings
logger:
  print_freq: 10    # 100
  save_checkpoint_freq: !!float 5000    # 5000
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29420

find_unused_parameters: true
